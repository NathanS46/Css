<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Style</title>
  </head>
  <style>
    body {
      background-image: url(back.jpg);
      background-size: 100%, auto;
      /* background-attachment: ; */
      background-repeat: repeat;
      padding-left: 50px;
      padding-right: 50px;
    }
    h1 {
      color: white;
    }
    p {
      color: black;
      text-align: center;
      padding: 50px;
      background-color: white;
      padding-left: 50px;
      padding-right: 50px;
      border-radius: 70px;
      font-size: 25px;
    }
  </style>
  <nav>
    <ul style="list-style: none">
      <li style="display: inline" id="nav-link">
        <a href="index.html">Home</a>
      </li>
      <li style="display: inline" id="nav-link">
        <a href="internal-css.html">Internal CSS page</a>
      </li>
      <li style="display: inline" id="nav-link">
        <a href="external-css.html">External CSS page</a>
      </li>
      <li style="display: inline" id="nav-link">
        <a href="testing-cascading.html">Testing Cascading page</a>
      </li>
      <li style="display: inline" id="nav-link">
        <a href="classes-and-ids.html">Classes and Id page</a>
      <li style="display: inline" id="nav-link">
        <a href="style-practice.html">Practice</a>
      <li style="display: inline" id="nav-link">
        <a href="style-practice2.html">Practice 2</a>
      </li>
    </ul>
  </nav>
  </head>
  <body>
    <h1>HELLo!</h1>
    <p>
      Take your creative projects to the next level with NVIDIA Studio. Powered
      by new dedicated hardware, RTX 40 Series unlocks unmatched performance in
      3D rendering, video editing, and graphic design. Experience feature-rich
      RTX accelerations in top creative apps, world-class NVIDIA Studio drivers
      engineered to provide maximum stability, and a suite of exclusive tools
      that harness the power of RTX for AI-assisted creative workflows.<br />
      <br />

      The open philosophy of wiki – allowing anyone to edit content – does not
      ensure that every editor's intentions are well-mannered. For example,
      vandalism (changing wiki content to something offensive, adding nonsense,
      maliciously removing content, or deliberately adding incorrect
      information, such as hoax information) can be a major problem. On larger
      wiki sites, such as those run by the Wikimedia Foundation, vandalism can
      go unnoticed for some period of time. Wikis, because of their open nature,
      are susceptible to intentional disruption, known as "trolling". Wikis tend
      to take a soft-security approach to the problem of vandalism, making
      damage easy to undo rather than attempting to prevent damage. Larger wikis
      often employ sophisticated methods, such as bots that automatically
      identify and revert vandalism and JavaScript enhancements that show
      characters that have been added in each edit. In this way, vandalism can
      be limited to just "minor vandalism" or "sneaky vandalism", where the
      characters added/eliminated are so few that bots do not identify them and
      users do not pay much attention to them.[29][unreliable source] An example
      of a bot that reverts vandalism on Wikipedia is ClueBot NG. ClueBot NG can
      revert edits, often within minutes, if not seconds. The bot uses machine
      learning in lieu of heuristics.[30]<br /><br />

      Malware can also be a problem for wikis, as users can add links to sites
      hosting malicious code. For example, a German Wikipedia article about the
      Blaster Worm was edited to include a hyperlink to a malicious website.
      Users of vulnerable Microsoft Windows systems who followed the link would
      be infected.[10] A countermeasure is the use of software that prevents
      users from saving an edit that contains a link to a site listed on a
      blacklist of malicious sites.<br /><br />

      GeForce is a brand of graphics processing units (GPUs) designed by Nvidia
      and marketed for the consumer market. As of the GeForce 40 series, there
      have been eighteen iterations of the design. The first GeForce products
      were discrete GPUs designed for add-on graphics boards, intended for the
      high-margin PC gaming market, and later diversification of the product
      line covered all tiers of the PC graphics market, ranging from
      cost-sensitive[1] GPUs integrated on motherboards, to mainstream add-in
      retail boards. Most recently, GeForce technology has been introduced into
      Nvidia's line of embedded application processors, designed for electronic
      handhelds and mobile handsets. With respect to discrete GPUs, found in
      add-in graphics-boards, Nvidia's GeForce and AMD's Radeon GPUs are the
      only remaining competitors in the high-end market. GeForce GPUs are very
      dominant in the general-purpose graphics processor unit (GPGPU) market
      thanks to their proprietary CUDA architecture.[2] GPGPU is expected to
      expand GPU functionality beyond the traditional rasterization of 3D
      graphics, to turn it into a high-performance computing device able to
      execute arbitrary programming code in the same way a CPU does, but with
      different strengths (highly parallel execution of straightforward
      calculations) and weaknesses (worse performance for complex branching
      code). Name origin The "GeForce" name originated from a contest held by
      Nvidia in early 1999 called "Name That Chip". The company called out to
      the public to name the successor to the RIVA TNT2 line of graphics boards.
      There were over 12,000 entries received and 7 winners received a RIVA TNT2
      Ultra graphics card as a reward.[3][4] Brian Burke, senior PR manager at
      Nvidia, told Maximum PC in 2002 that "GeForce" originally stood for
      "Geometry Force" since GeForce 256 was the first GPU for personal
      computers to calculate the transform-and-lighting geometry, offloading
      that function from the CPU.[5] Graphics processor generations Generations
      timeline 1999 GeForce 256 2000 GeForce 2 series 2001 GeForce 3 series 2002
      GeForce 4 series 2003 GeForce FX series 2004 GeForce 6 series 2005 GeForce
      7 series 2006 GeForce 8 series 2007 2008 GeForce 9 series GeForce 200
      series 2009 GeForce 100 series GeForce 300 series 2010 GeForce 400 series
      GeForce 500 series 2011 2012 GeForce 600 series 2013 GeForce 700 series
      2014 GeForce 800M series GeForce 900 series 2015 2016 GeForce 10 series
      2017 2018 GeForce 20 series 2019 GeForce 16 series 2020 GeForce 30 series
      2021 2022 GeForce 40 series GeForce 256 Main article: GeForce 256 GeForce
      2 series Main article: GeForce 2 series Launched in April 2000, the first
      GeForce2 (NV15) was another high-performance graphics chip. Nvidia moved
      to a twin texture processor per pipeline (4x2) design, doubling texture
      fillrate per clock compared to GeForce 256. Later, Nvidia released the
      GeForce2 MX (NV11), which offered performance similar to the GeForce 256
      but at a fraction of the cost. The MX was a compelling value in the
      low/mid-range market segments and was popular with OEM PC manufacturers
      and users alike. The GeForce 2 Ultra was the high-end model in this
      series. GeForce 3 series Main article: GeForce 3 series Launched in
      February 2001, the GeForce3 (NV20) introduced programmable vertex and
      pixel shaders to the GeForce family and to consumer-level graphics
      accelerators. It had good overall performance and shader support, making
      it popular with enthusiasts although it never hit the midrange price
      point. The NV2A developed for the Microsoft Xbox game console is a
      derivative of the GeForce 3. GeForce 4 series Main article: GeForce 4
      series Launched in February 2002, the then-high-end GeForce4 Ti (NV25) was
      mostly a refinement to the GeForce3. The biggest advancements included
      enhancements to anti-aliasing capabilities, an improved memory controller,
      a second vertex shader, and a manufacturing process size reduction to
      increase clock speeds. Another member of the GeForce 4 family, the budget
      GeForce4 MX was based on the GeForce2, with the addition of some features
      from the GeForce4 Ti. It targeted the value segment of the market and
      lacked pixel shaders. Most of these models used the AGP 4× interface, but
      a few began the transition to AGP 8×. GeForce FX series Main article:
      GeForce FX series Launched in 2003, the GeForce FX (NV30) was a huge
      change in architecture compared to its predecessors. The GPU was designed
      not only to support the new Shader Model 2 specification but also to
      perform well on older titles. However, initial models like the GeForce FX
      5800 Ultra suffered from weak floating point shader performance and
      excessive heat which required infamously noisy two-slot cooling solutions.
      Products in this series carry the 5000 model number, as it is the fifth
      generation of the GeForce, though Nvidia marketed the cards as GeForce FX
      instead of GeForce 5 to show off "the dawn of cinematic rendering".
      GeForce 6 series Main article: GeForce 6 series Launched in April 2004,
      the GeForce 6 (NV40) added Shader Model 3.0 support to the GeForce family,
      while correcting the weak floating point shader performance of its
      predecessor. It also implemented high-dynamic-range imaging and introduced
      SLI (Scalable Link Interface) and PureVideo capability (integrated partial
      hardware MPEG-2, VC-1, Windows Media Video, and H.264 decoding and fully
      accelerated video post-processing). GeForce 7 series Main article: GeForce
      7 series The seventh generation GeForce (G70/NV47) was launched in June
      2005 and was the last Nvidia video card series that could support the AGP
      bus. The design was a refined version of GeForce 6, with the major
      improvements being a widened pipeline and an increase in clock speed. The
      GeForce 7 also offers new transparency supersampling and transparency
      multisampling anti-aliasing modes (TSAA and TMAA). These new anti-aliasing
      modes were later enabled for the GeForce 6 series as well. The GeForce
      7950GT featured the highest performance GPU with an AGP interface in the
      Nvidia line. This era began the transition to the PCI-Express interface. A
      128-bit, 8 ROP variant of the 7800 GTX, called the RSX 'Reality
      Synthesizer', is used as the main GPU in the Sony PlayStation 3. GeForce 8
      series Main article: GeForce 8 series Released on November 8, 2006, the
      eighth-generation GeForce (originally called G80) was the first ever GPU
      to fully support Direct3D 10. Manufactured using a 90 nm process and built
      around the new Tesla microarchitecture, it implemented the unified shader
      model. Initially just the 8800GTX model was launched, while the GTS
      variant was released months into the product line's life, and it took
      nearly six months for mid-range and OEM/mainstream cards to be integrated
      into the 8 series. The die shrink down to 65 nm and a revision to the G80
      design, codenamed G92, were implemented into the 8 series with the 8800GS,
      8800GT and 8800GTS-512, first released on October 29, 2007, almost one
      whole year after the initial G80 release. GeForce 9 series and 100 series
      Main articles: GeForce 9 series and GeForce 100 series The first product
      was released on February 21, 2008.[6] Not even four months older than the
      initial G92 release, all 9-series designs are simply revisions to existing
      late 8-series products. The 9800GX2 uses two G92 GPUs, as used in later
      8800 cards, in a dual PCB configuration while still only requiring a
      single PCI-Express 16x slot. The 9800GX2 utilizes two separate 256-bit
      memory busses, one for each GPU and its respective 512 MB of memory, which
      equates to an overall of 1 GB of memory on the card (although the SLI
      configuration of the chips necessitates mirroring the frame buffer between
      the two chips, thus effectively halving the memory performance of a
      256-bit/512MB configuration). The later 9800GTX features a single G92 GPU,
      256-bit data bus, and 512 MB of GDDR3 memory.[7] Prior to the release, no
      concrete information was known except that the officials claimed the next
      generation products had close to 1 TFLOPS processing power with the GPU
      cores still being manufactured in the 65 nm process, and reports about
      Nvidia downplaying the significance of Direct3D 10.1.[8] In March 2009,
      several sources reported that Nvidia had quietly launched a new series of
      GeForce products, namely the GeForce 100 Series, which consists of
      rebadged 9 Series parts.[9][10][11] GeForce 100 series products were not
      available for individual purchase.[1] GeForce 200 series and 300 series
      Main articles: GeForce 200 series and GeForce 300 series Based on the
      GT200 graphics processor consisting of 1.4 billion transistors, codenamed
      Tesla, the 200 series was launched on June 16, 2008.[12] The next
      generation of the GeForce series takes the card-naming scheme in a new
      direction, by replacing the series number (such as 8800 for 8-series
      cards) with the GTX or GTS suffix (which used to go at the end of card
      names, denoting their 'rank' among other similar models), and then adding
      model-numbers such as 260 and 280 after that. The series features the new
      GT200 core on a 65nm die.[13] The first products were the GeForce GTX 260
      and the more expensive GeForce GTX 280.[14] The GeForce 310 was released
      on November 27, 2009, which is a rebrand of GeForce 210.[15][16] The 300
      series cards are rebranded DirectX 10.1 compatible GPUs from the 200
      series, which were not available for individual purchase. GeForce 400
      series and 500 series Main articles: GeForce 400 series and GeForce 500
      series On April 7, 2010, Nvidia released[17] the GeForce GTX 470 and GTX
      480, the first cards based on the new Fermi architecture, codenamed GF100;
      they were the first Nvidia GPUs to utilize 1 GB or more of GDDR5 memory.
      The GTX 470 and GTX 480 were heavily criticized due to high power use,
      high temperatures, and very loud noise that were not balanced by the
      performance offered, even though the GTX 480 was the fastest DirectX 11
      card as of its introduction. In November 2010, Nvidia released a new
      flagship GPU based on an enhanced GF100 architecture (GF110) called the
      GTX 580. It featured higher performance, less power utilization, heat and
      noise than the preceding GTX 480. This GPU received much better reviews
      than the GTX 480. Nvidia later also released the GTX 590, which packs two
      GF110 GPUs on a single card.
    </p>
  </body>
</html>
